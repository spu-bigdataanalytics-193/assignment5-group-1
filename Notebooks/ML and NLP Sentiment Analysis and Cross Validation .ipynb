{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK for performing NLP Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CDJXoXQCxTys",
    "outputId": "03c6034a-c616-46ce-f0c1-bc7ba6ee2a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisties and Libraries Required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNh5Ka44xdnO"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hPBYMJA3xhio",
    "outputId": "4eaae535-bcbb-45d5-9118-ed8c9abc1e8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words_clean: array<string>, doc_id: bigint]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens.withColumn(\"doc_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "Lh2SCQQo0UeO",
    "outputId": "41da7e0b-c099-4407-e598-6a694434c985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         words_clean|\n",
      "+--------------------+\n",
      "|[ive, read, book,...|\n",
      "|[nicely, written,...|\n",
      "|              [love]|\n",
      "|[good, additional...|\n",
      "|[gazillion, patte...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_tokens.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gp14sgtWQtFX"
   },
   "source": [
    "# Preparing for Machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsyvcPvQF09F"
   },
   "source": [
    "# TF - IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKi-LfQsQxiM"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "V4qmCKolVU8i",
    "outputId": "0ae714ee-7088-4c58-bc22-770f5cf16852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    5.0|I've read this bo...|\n",
      "|    5.0|Nicely written di...|\n",
      "|    5.0|             love it|\n",
      "|    5.0|Good additional k...|\n",
      "|    5.0|A gazillion patte...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data with overall scores\n",
    "data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qqhkp6BoVWpx"
   },
   "outputs": [],
   "source": [
    "data_clean1 = data_clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # SENTIMENT ANALYSIS \n",
    "   \n",
    "###   Sentiment analysis or sentiment classification fall into the broad category of text classification tasks where you are supplied with a phrase, or a list of phrases and your classifier is supposed to tell if the sentiment behind that is positive, negative or neutral. Sometimes, the third attribute is not taken to keep it a binary classification problem. In recent tasks, sentiments like \"somewhat positive\" and \"somewhat negative\" are also being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understading the reviewText based on positive, negative or neutral emotion of the sentence. Here we bifurcate the rating score based on \n",
    "# Score > 3 stars as positive, \n",
    "# Score < 3 star as negative and \n",
    "# Score = 3 star as neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "colab_type": "code",
    "id": "TpnQsiuPXKym",
    "outputId": "26e0fb1c-d305-4cfb-8196-c9987d7baa2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------+\n",
      "|overall|          reviewText|      sentiment|\n",
      "+-------+--------------------+---------------+\n",
      "|    5.0|I've read this bo...|Positive Review|\n",
      "|    5.0|Nicely written di...|Positive Review|\n",
      "|    5.0|             love it|Positive Review|\n",
      "|    5.0|Good additional k...|Positive Review|\n",
      "|    5.0|A gazillion patte...|Positive Review|\n",
      "|    2.0|Just ok. Read bet...|Negative Review|\n",
      "|    5.0|The best knitting...|Positive Review|\n",
      "|    5.0|This book is a mo...|Positive Review|\n",
      "|    5.0|excellent variety...|Positive Review|\n",
      "|    5.0|Another winner M....|Positive Review|\n",
      "|    5.0|Love all the patt...|Positive Review|\n",
      "|    4.0|Good selection of...|Positive Review|\n",
      "|    4.0|Contains some int...|Positive Review|\n",
      "|    5.0|Super useful for ...|Positive Review|\n",
      "|    5.0|Loved it. Differe...|Positive Review|\n",
      "|    5.0|I love the book b...|Positive Review|\n",
      "|    5.0|Awesome book!!! A...|Positive Review|\n",
      "|    5.0|   Lots of stitches.|Positive Review|\n",
      "|    5.0|Easy to follow al...|Positive Review|\n",
      "|    5.0|I'm a fairly expe...|Positive Review|\n",
      "+-------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = data_clean1.withColumn(\"sentiment\",when(col(\"overall\") > '3.0' , 'Positive Review')\n",
    "    .otherwise(when(col(\"overall\") == '3.0', 'Neutral Review')\n",
    "        .otherwise('Negative Review')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQEy2tglZqZ9"
   },
   "outputs": [],
   "source": [
    "dt2 = data_clean1.withColumn(\"sentiment\",when(col(\"overall\") > '3.0' , 'Positive Review')\n",
    "    .otherwise(when(col(\"overall\") == '3.0', 'Neutral Review')\n",
    "        .otherwise('Negative Review')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIVIDING THE DATA INTO TRAIN, TEST AND VALIDATION SET TO TRAIN OUR MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdBGlvC4Wku1"
   },
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = dt2.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. Tokenizer will tokenize the sentenses into tokens and then the model performs the TD and IDF methods storing them in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "C7e_viBAZV9-",
    "outputId": "84c4c664-95ea-4b2a-9aa7-4585b4dc3dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------+--------------------+--------------------+--------------------+-----+\n",
      "|overall|          reviewText|      sentiment|               words|                  tf|            features|label|\n",
      "+-------+--------------------+---------------+--------------------+--------------------+--------------------+-----+\n",
      "|    1.0|\n",
      "\n",
      "I'm like the ot...|Negative Review|[, , i'm, like, t...|(65536,[4492,5887...|(65536,[4492,5887...|  2.0|\n",
      "|    1.0|\n",
      "Very disappointe...|Negative Review|[, very, disappoi...|(65536,[4917,8026...|(65536,[4917,8026...|  2.0|\n",
      "|    1.0|       .\n",
      " ..\n",
      ", .....|Negative Review|[, , , , , , , .,...|(65536,[1536,1438...|(65536,[1536,1438...|  2.0|\n",
      "|    1.0|  from Dorian on ...|Negative Review|[, , from, dorian...|(65536,[14,158,18...|(65536,[14,158,18...|  2.0|\n",
      "|    1.0| Flimsy, metal is...|Negative Review|[, flimsy,, metal...|(65536,[1053,2712...|(65536,[1053,2712...|  2.0|\n",
      "+-------+--------------------+---------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"overall\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "### Logistic Regression using Count Vector Features\n",
    "### Our model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGDQGOVzaKza"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction | Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LeADE2KvbpYk",
    "outputId": "7afdecee-7abf-4df2-d5b7-8aa7db685734"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.87384758923"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "BSfUNSO4dFu3",
    "outputId": "18c0576b-0c31-473f-dc10-485830e575d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 92.8738\n",
      "ROC-AUC: 0.9678\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"overall\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Let’s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "frhOjKgZijpn",
    "outputId": "67d03faf-f88b-4309-d54c-ddd6ae194cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95836473837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(dt2)\n",
    "dataset = pipelineFit.transform(dt2)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(train_set)\n",
    "\n",
    "predictions = cvModel.transform(test_set)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n",
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRw6X-PHs7VL"
   },
   "source": [
    "## Accuracy Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "610_FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
